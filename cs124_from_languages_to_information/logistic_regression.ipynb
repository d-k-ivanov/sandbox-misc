{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math\r\n",
    "\r\n",
    "def sigmoid(x):\r\n",
    "  return 1 / (1 + math.exp(-x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def dot_product(a_vector, b_vector):\r\n",
    "    # a1 x b1 + a2 * b2..an*bn return scalar\r\n",
    "    return sum([an * bn for an, bn in zip(a_vector,b_vector)])\r\n",
    "\r\n",
    "x = [2.5,-5,-1.2,0.5,2,0.7]\r\n",
    "Y = [3,2,1,3,0,4.19]\r\n",
    "\r\n",
    "print(round(dot_product(x,Y), 5))\r\n",
    "\r\n",
    "classified_1 = round(dot_product(x,Y), 5) + 0.1\r\n",
    "prob_1 = sigmoid(classified_1)\r\n",
    "print(round(prob_1, 2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "x = [2.5,-5,-1.2,0.5,2,0.7]\r\n",
    "Y = [3,2,1,3,0,4.19]\r\n",
    "\r\n",
    "result = np.dot(x, Y)\r\n",
    "print(round(result, 5))\r\n",
    "\r\n",
    "classified_2 = round(result, 5) + 0.1\r\n",
    "prob_2 = sigmoid(classified_1)\r\n",
    "n_prob_2 = 1 - sigmoid(classified_1)\r\n",
    "print(round(prob_2, 2))\r\n",
    "print(round(n_prob_2, 2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Loss\r\n",
    "import math\r\n",
    "import numpy\r\n",
    "\r\n",
    "def sigmoid(x):\r\n",
    "    return 1 / (1 + math.exp(-x))\r\n",
    "\r\n",
    "result = np.dot(x, Y)\r\n",
    "classified_2 = round(result, 5) + 0.1\r\n",
    "p_prob = sigmoid(classified_1)\r\n",
    "n_prob = 1 - sigmoid(classified_1)\r\n",
    "print(f\"Positive probability: {round(p_prob, 2)}\")\r\n",
    "print(f\"Negative probability: {round(n_prob, 2)}\")\r\n",
    "\r\n",
    "# y = 1\r\n",
    "# LCE = -[y * log(sigmoid(np.dot(x, Y) + 0.1) + (1 - y) * log(1 - sigmoid(np.dot(x, Y) + 0.1))]\r\n",
    "# LCE = -[1 * log(sigmoid(np.dot(x, Y) + 0.1) + (1 - 1) * log(1 - sigmoid(np.dot(x, Y) + 0.1))]\r\n",
    "# LCE = -[log(sigmoid(np.dot(x, Y) + 0.1) + 0]\r\n",
    "# LCE = -[log(sigmoid(np.dot(x, Y) + 0.1)]\r\n",
    "\r\n",
    "p_lce = -(math.log(round(p_prob, 2)))\r\n",
    "print(f\"Cross-entropy loss (positive prediction y=1): {round(p_lce, 2)}\")\r\n",
    "\r\n",
    "# y = 0\r\n",
    "# LCE = -[y * log(sigmoid(np.dot(x, Y) + 0.1) + (1 - y) * log(1 - sigmoid(np.dot(x, Y) + 0.1))]\r\n",
    "# LCE = -[0 * log(sigmoid(np.dot(x, Y) + 0.1) + (1 - 0) * log(1 - sigmoid(np.dot(x, Y) + 0.1))]\r\n",
    "# LCE = -[0 + (1) * log(1 - sigmoid(np.dot(x, Y) + 0.1))]\r\n",
    "# LCE = -[log(1 - sigmoid(np.dot(x, Y) + 0.1))]\r\n",
    "n_lce = -(math.log(round(n_prob, 2)))\r\n",
    "print(f\"Cross-entropy loss (negative prediction y=0): {round(n_lce, 2)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def sigmoid_range(x):\r\n",
    "    a = []\r\n",
    "    for item in x:\r\n",
    "        a.append(1 / (1 + math.exp(-item)))\r\n",
    "    return a\r\n",
    "\r\n",
    "x = np.arange(-10., 10., 0.1)\r\n",
    "sig = sigmoid_range(x)\r\n",
    "plt.plot(x,sig)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import csv\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "\r\n",
    "def loadCSV(filename):\r\n",
    "    '''\r\n",
    "    function to load dataset\r\n",
    "    '''\r\n",
    "    with open(filename,\"r\") as csvfile:\r\n",
    "        lines = csv.reader(csvfile)\r\n",
    "        dataset = list(lines)\r\n",
    "        for i in range(len(dataset)):\r\n",
    "            dataset[i] = [float(x) for x in dataset[i]]\r\n",
    "    return np.array(dataset)\r\n",
    "\r\n",
    "\r\n",
    "def normalize(x):\r\n",
    "    '''\r\n",
    "    function to normalize feature matrix, x\r\n",
    "    '''\r\n",
    "    mins = np.min(x, axis = 0)\r\n",
    "    maxs = np.max(x, axis = 0)\r\n",
    "    rng = maxs - mins\r\n",
    "    norm_x = 1 - ((maxs - x)/rng)\r\n",
    "    return norm_x\r\n",
    "\r\n",
    "\r\n",
    "def logistic_func(theta, x):\r\n",
    "    '''\r\n",
    "    logistic(sigmoid) function 1/1+e^-theta.T * x\r\n",
    "    '''\r\n",
    "    return 1.0/(1 + np.exp(-np.dot(x, theta.T)))\r\n",
    "\r\n",
    "\r\n",
    "def log_gradient(theta, x, y):\r\n",
    "    '''\r\n",
    "    logistic gradient function [Y(PREDICTION)- Y(ACTUAL)).Transpose *  x\r\n",
    "    '''\r\n",
    "    first_calc = logistic_func(theta, x) - y.reshape(x.shape[0], -1)\r\n",
    "    final_calc = np.dot(first_calc.T, x)\r\n",
    "    return final_calc\r\n",
    "\r\n",
    "\r\n",
    "def cost_func(theta, x, y):\r\n",
    "    '''\r\n",
    "    cost function, J\r\n",
    "    '''\r\n",
    "    log_func_v = logistic_func(theta, x)\r\n",
    "    y = np.squeeze(y)\r\n",
    "    step1 = y * np.log(log_func_v)\r\n",
    "    step2 = (1 - y) * np.log(1 - log_func_v)\r\n",
    "    final = -step1 - step2\r\n",
    "    return np.mean(final)\r\n",
    "\r\n",
    "\r\n",
    "def grad_desc(x, y, theta, lr=.001, converge_change=.001):\r\n",
    "    '''\r\n",
    "    gradient descent function\r\n",
    "    '''\r\n",
    "    cost = cost_func(theta, x, y)\r\n",
    "    change_cost = 1\r\n",
    "    num_iter = 1\r\n",
    "\r\n",
    "    while(change_cost > converge_change):\r\n",
    "        old_cost = cost\r\n",
    "        theta = theta - (lr * log_gradient(theta, x, y))\r\n",
    "        cost = cost_func(theta, x, y)\r\n",
    "        change_cost = old_cost - cost\r\n",
    "        num_iter += 1\r\n",
    "\r\n",
    "    return theta, num_iter\r\n",
    "\r\n",
    "\r\n",
    "def pred_values(theta, x):\r\n",
    "    '''\r\n",
    "    function to predict labels\r\n",
    "    '''\r\n",
    "    pred_prob = logistic_func(theta, x)\r\n",
    "    pred_value = np.where(pred_prob >= .5, 1, 0)\r\n",
    "    return np.squeeze(pred_value)\r\n",
    "\r\n",
    "\r\n",
    "def plot_reg(x, y, theta):\r\n",
    "    '''\r\n",
    "    function to plot decision boundary\r\n",
    "    '''\r\n",
    "    # labelled observations\r\n",
    "    x_0 = x[np.where(y == 0.0)]\r\n",
    "    x_1 = x[np.where(y == 1.0)]\r\n",
    "\r\n",
    "    # plotting points with diff color for diff label\r\n",
    "    plt.scatter([x_0[:, 1]], [x_0[:, 2]], c='b', label='y = 0')\r\n",
    "    plt.scatter([x_1[:, 1]], [x_1[:, 2]], c='r', label='y = 1')\r\n",
    "\r\n",
    "    # plotting decision boundary\r\n",
    "    x1 = np.arange(0, 1, 0.1)\r\n",
    "    x2 = -(theta[0,0] + theta[0,1]*x1)/theta[0,2]\r\n",
    "    plt.plot(x1, x2, c='k', label='reg line')\r\n",
    "\r\n",
    "    plt.xlabel('x1')\r\n",
    "    plt.ylabel('x2')\r\n",
    "    plt.legend()\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "# load the dataset\r\n",
    "dataset = loadCSV('./datasets/logistic.csv')\r\n",
    "\r\n",
    "# normalizing feature matrix\r\n",
    "x = normalize(dataset[:, :-1])\r\n",
    "print(x)\r\n",
    "\r\n",
    "# stacking columns wth all ones in feature matrix\r\n",
    "x = np.hstack((np.matrix(np.ones(x.shape[0])).T, x))\r\n",
    "\r\n",
    "# response vector\r\n",
    "y = dataset[:, -1]\r\n",
    "print (y)\r\n",
    "\r\n",
    "# initial beta values\r\n",
    "theta = np.matrix(np.zeros(x.shape[1]))\r\n",
    "print (theta )\r\n",
    "# beta values after running gradient descent\r\n",
    "theta, num_iter = grad_desc(x, y, theta)\r\n",
    "\r\n",
    "# estimated beta values and number of iterations\r\n",
    "print(\"Estimated regression coefficients:\", theta)\r\n",
    "print(\"No. of iterations:\", num_iter)\r\n",
    "\r\n",
    "# predicted labels\r\n",
    "y_pred = pred_values(theta, x)\r\n",
    "\r\n",
    "# number of correctly predicted labels\r\n",
    "print(\"Correctly predicted labels:\", np.sum(y == y_pred))\r\n",
    "\r\n",
    "# plotting regression line\r\n",
    "plot_reg(x, y, theta)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\r\n",
    "from sklearn.gaussian_process.kernels import RBF\r\n",
    "from sklearn import datasets\r\n",
    "\r\n",
    "iris = datasets.load_iris()\r\n",
    "x = iris.data[:, 0:2]  # we only take the first two features for visualization\r\n",
    "y = iris.target\r\n",
    "\r\n",
    "n_features = x.shape[1]\r\n",
    "\r\n",
    "C = 10\r\n",
    "kernel = 1.0 * RBF([1.0, 1.0])  # for GPC\r\n",
    "\r\n",
    "# Create different classifiers.\r\n",
    "classifiers = {\r\n",
    "    'L1 logistic': LogisticRegression(C=C, penalty='l1',\r\n",
    "                                      solver='saga',\r\n",
    "                                      multi_class='multinomial',\r\n",
    "                                      max_iter=10000),\r\n",
    "    'L2 logistic (Multinomial)': LogisticRegression(C=C, penalty='l2',\r\n",
    "                                                    solver='saga',\r\n",
    "                                                    multi_class='multinomial',\r\n",
    "                                                    max_iter=10000),\r\n",
    "    'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2',\r\n",
    "                                            solver='saga',\r\n",
    "                                            multi_class='ovr',\r\n",
    "                                            max_iter=10000),\r\n",
    "    'Linear SVC': SVC(kernel='linear', C=C, probability=True,\r\n",
    "                      random_state=0),\r\n",
    "    'GPC': GaussianProcessClassifier(kernel)\r\n",
    "}\r\n",
    "\r\n",
    "n_classifiers = len(classifiers)\r\n",
    "\r\n",
    "plt.figure(figsize=(3 * 2, n_classifiers * 2))\r\n",
    "plt.subplots_adjust(bottom=.2, top=.95)\r\n",
    "\r\n",
    "xx = np.linspace(3, 9, 100)\r\n",
    "yy = np.linspace(1, 5, 100).T\r\n",
    "xx, yy = np.meshgrid(xx, yy)\r\n",
    "xfull = np.c_[xx.ravel(), yy.ravel()]\r\n",
    "\r\n",
    "for index, (name, classifier) in enumerate(classifiers.items()):\r\n",
    "    classifier.fit(x, y)\r\n",
    "\r\n",
    "    y_pred = classifier.predict(x)\r\n",
    "    accuracy = accuracy_score(y, y_pred)\r\n",
    "    print(\"Accuracy (train) for %s: %0.1f%% \" % (name, accuracy * 100))\r\n",
    "\r\n",
    "    # View probabilities:\r\n",
    "    probas = classifier.predict_proba(xfull)\r\n",
    "    n_classes = np.unique(y_pred).size\r\n",
    "    for k in range(n_classes):\r\n",
    "        plt.subplot(n_classifiers, n_classes, index * n_classes + k + 1)\r\n",
    "        plt.title(\"Class %d\" % k)\r\n",
    "        if k == 0:\r\n",
    "            plt.ylabel(name)\r\n",
    "        imshow_handle = plt.imshow(probas[:, k].reshape((100, 100)),\r\n",
    "                                   extent=(3, 9, 1, 5), origin='lower')\r\n",
    "        plt.xticks(())\r\n",
    "        plt.yticks(())\r\n",
    "        idx = (y_pred == k)\r\n",
    "        if idx.any():\r\n",
    "            plt.scatter(x[idx, 0], x[idx, 1], marker='o', c='w', edgecolor='k')\r\n",
    "\r\n",
    "ax = plt.axes([0.15, 0.04, 0.7, 0.05])\r\n",
    "plt.title(\"Probability\")\r\n",
    "plt.colorbar(imshow_handle, cax=ax, orientation='horizontal')\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from time import time\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from sklearn import linear_model\r\n",
    "from sklearn import datasets\r\n",
    "from sklearn.svm import l1_min_c\r\n",
    "\r\n",
    "iris = datasets.load_iris()\r\n",
    "x = iris.data\r\n",
    "y = iris.target\r\n",
    "\r\n",
    "x = x[y != 2]\r\n",
    "y = y[y != 2]\r\n",
    "\r\n",
    "x /= x.max()  # Normalize x to speed-up convergence\r\n",
    "\r\n",
    "cs = l1_min_c(x, y, loss='log') * np.logspace(0, 7, 16)\r\n",
    "\r\n",
    "print(\"Computing regularization path ...\")\r\n",
    "start = time()\r\n",
    "clf = linear_model.LogisticRegression(penalty='l1', solver='liblinear',\r\n",
    "                                      tol=1e-6, max_iter=int(1e6),\r\n",
    "                                      warm_start=True,\r\n",
    "                                      intercept_scaling=10000.)\r\n",
    "coefs_ = []\r\n",
    "for c in cs:\r\n",
    "    clf.set_params(C=c)\r\n",
    "    clf.fit(x, y)\r\n",
    "    coefs_.append(clf.coef_.ravel().copy())\r\n",
    "print(\"This took %0.3fs\" % (time() - start))\r\n",
    "\r\n",
    "coefs_ = np.array(coefs_)\r\n",
    "plt.plot(np.log10(cs), coefs_, marker='o')\r\n",
    "ymin, ymax = plt.ylim()\r\n",
    "plt.xlabel('log(C)')\r\n",
    "plt.ylabel('Coefficients')\r\n",
    "plt.title('Logistic Regression Path')\r\n",
    "plt.axis('tight')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit (system)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "d4be04134da666ba417300a0c020abeb8478be2ead96ebfe9eeadc929de095c9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
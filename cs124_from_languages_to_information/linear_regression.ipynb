{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MyLinearRegression:\r\n",
    "    def __init__(self, weight=1, bias=2, learning_rate=0.0005, iterations=80):\r\n",
    "        self.weight = weight\r\n",
    "        self.bias = bias\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.iterations = iterations\r\n",
    "        self.cost_trend = []\r\n",
    "        self.cost = 0\r\n",
    "\r\n",
    "    def predict(self, x):\r\n",
    "        predicted_set = []\r\n",
    "        for i in range(len(x)):\r\n",
    "            predicted_value = self.weight * x[i] + self.bias\r\n",
    "            predicted_set.append(predicted_value)\r\n",
    "        return predicted_set\r\n",
    "\r\n",
    "    def cost_function(self, x, y):\r\n",
    "        count = len(x)\r\n",
    "        total_error = 0.0\r\n",
    "        for i in range(count):\r\n",
    "            total_error += (y[i] - (self.weight * x[i] +\r\n",
    "                            self.bias)) ** 2\r\n",
    "        return float(total_error) / (2 * count)\r\n",
    "\r\n",
    "    def update_weights(self, x, y):\r\n",
    "        weight_deriv = 0\r\n",
    "        bias_deriv = 0\r\n",
    "        count = len(x)\r\n",
    "\r\n",
    "        for i in range(count):\r\n",
    "            # Calculate partial derivatives\r\n",
    "            # -2x(y - (mx + b))\r\n",
    "            weight_deriv += -2 * x[i] * (y[i] -(self.weight * x[i] + self.bias))\r\n",
    "\r\n",
    "            # -2(y - (mx + b))\r\n",
    "            bias_deriv += -2 * (y[i] - (self.weight * x[i] +\r\n",
    "                                self.bias))\r\n",
    "\r\n",
    "        # We subtract because the derivatives point in direction of steepest\r\n",
    "        # ascent\r\n",
    "        self.weight -= (weight_deriv / count) * self.learning_rate\r\n",
    "        self.bias -= (bias_deriv / count) * self.learning_rate\r\n",
    "\r\n",
    "    def train(self, x, y):\r\n",
    "        for i in range(self.iterations):\r\n",
    "            self.update_weights(x, y)\r\n",
    "            # Calculating cost\r\n",
    "            self.cost = self.cost_function(x, y)\r\n",
    "            self.cost_trend.append(self.cost)\r\n",
    "           # if i % 10000 == 0:\r\n",
    "            print(\"Iteration: {}\\t Weight: {}\\t Bias: {}\\t Cost: {}\".format(i, self.weight, self.bias, self.cost))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# intialise data of lists.\r\n",
    "data = {'Hours':[2.5,5.1,3.2,8.5,3.5,1.5,9.2,5.5,8.3,2.7,7.7,5.9,4.5,3.3,1.1,8.9,2.5,1.9,6.1,7.4,2.7,4.8,3.8,6.9,7.8],\r\n",
    "        'Scores':[21,47,27,75,30,20,88,60,81,25,85,62,41,42,17,95,30,24,67,69,30,54,35,76,86]}\r\n",
    "\r\n",
    "# Create DataFrame\r\n",
    "studentscores = pd.DataFrame(data)\r\n",
    "\r\n",
    "# Print the output.\r\n",
    "studentscores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x=[2.5,5.1,3.2,8.5,3.5,1.5,9.2,5.5,8.3,2.7,7.7,5.9,4.5,3.3,1.1,8.9,2.5,1.9,6.1,7.4,2.7,4.8,3.8,6.9,7.8] \r\n",
    "y=[21,47,27,75,30,20,88,60,81,25,85,62,41,42,17,95,30,24,67,69,30,54,35,76,86]\r\n",
    "plt.scatter(x,y,s=10)\r\n",
    "plt.xlabel('x')\r\n",
    "plt.ylabel('y')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "x = studentscores.iloc[:, :-1].values\r\n",
    "y = studentscores.iloc[:, -1].values\r\n",
    "x,y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/3, random_state=0)\r\n",
    "\r\n",
    "# Fitting Simple Linear Regression to the Training set\r\n",
    "regressor = MyLinearRegression()\r\n",
    "regressor.train(x_train, y_train)\r\n",
    "print('Weight: ' + str(regressor.weight) + ' Bias: ' + str(regressor.bias))\r\n",
    "\r\n",
    "# Predicting the Test set results\r\n",
    "y_pred = regressor.predict(x_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x=[int(x) for x in range(80)]\r\n",
    "y=regressor.cost_trend\r\n",
    "plt.plot(x,y)\r\n",
    "plt.xlabel('x')\r\n",
    "plt.ylabel('y')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w = regressor.weight\r\n",
    "b = regressor.bias\r\n",
    "x=[2.5,5.1,3.2,8.5,3.5,1.5,9.2,5.5,8.3,2.7,7.7,5.9,4.5,3.3,1.1,8.9,2.5,1.9,6.1,7.4,2.7,4.8,3.8,6.9,7.8]\r\n",
    "y=[21,47,27,75,30,20,88,60,81,25,85,62,41,42,17,95,30,24,67,69,30,54,35,76,86]\r\n",
    "plt.scatter(x,y)\r\n",
    "axes = plt.gca()\r\n",
    "x_vals = np.array(axes.get_xlim())\r\n",
    "y_vals = b + w * x_vals\r\n",
    "plt.plot(x_vals, y_vals)\r\n",
    "plt.xlabel('x')\r\n",
    "plt.ylabel('y')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.pipeline import make_pipeline\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "rng = np.random.RandomState(0)\r\n",
    "n_samples = 500\r\n",
    "cov = [[3, 3],\r\n",
    "       [3, 4]]\r\n",
    "x = rng.multivariate_normal(mean=[0, 0], cov=cov, size=n_samples)\r\n",
    "pca = PCA(n_components=2).fit(x)\r\n",
    "\r\n",
    "\r\n",
    "plt.scatter(x[:, 0], x[:, 1], alpha=.3, label='samples')\r\n",
    "for i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):\r\n",
    "    comp = comp * var  # scale component by its variance explanation power\r\n",
    "    plt.plot([0, comp[0]], [0, comp[1]], label=f\"Component {i}\", linewidth=5,\r\n",
    "             color=f\"C{i + 2}\")\r\n",
    "plt.gca().set(aspect='equal',\r\n",
    "              title=\"2-dimensional dataset with principal components\",\r\n",
    "              xlabel='first feature', ylabel='second feature')\r\n",
    "\r\n",
    "y = x.dot(pca.components_[1]) + rng.normal(size=n_samples) / 2\r\n",
    "\r\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\r\n",
    "\r\n",
    "axes[0].scatter(x.dot(pca.components_[0]), y, alpha=.3)\r\n",
    "axes[0].set(xlabel='Projected data onto first PCA component', ylabel='y')\r\n",
    "axes[1].scatter(x.dot(pca.components_[1]), y, alpha=.3)\r\n",
    "axes[1].set(xlabel='Projected data onto second PCA component', ylabel='y')\r\n",
    "\r\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=rng)\r\n",
    "\r\n",
    "pcr = make_pipeline(StandardScaler(), PCA(n_components=1), LinearRegression())\r\n",
    "pcr.fit(x_train, y_train)\r\n",
    "pca = pcr.named_steps['pca']  # retrieve the PCA step of the pipeline\r\n",
    "\r\n",
    "pls = PLSRegression(n_components=1)\r\n",
    "pls.fit(x_train, y_train)\r\n",
    "\r\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\r\n",
    "axes[0].scatter(pca.transform(x_test), y_test, alpha=.3, label='ground truth')\r\n",
    "axes[0].scatter(pca.transform(x_test), pcr.predict(x_test), alpha=.3,\r\n",
    "                label='predictions')\r\n",
    "axes[0].set(xlabel='Projected data onto first PCA component',\r\n",
    "            ylabel='y', title='PCR / PCA')\r\n",
    "axes[0].legend()\r\n",
    "axes[1].scatter(pls.transform(x_test), y_test, alpha=.3, label='ground truth')\r\n",
    "axes[1].scatter(pls.transform(x_test), pls.predict(x_test), alpha=.3,\r\n",
    "                label='predictions')\r\n",
    "axes[1].set(xlabel='Projected data onto first PLS component',\r\n",
    "            ylabel='y', title='PLS')\r\n",
    "axes[1].legend()\r\n",
    "plt.tight_layout()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.metrics import r2_score\r\n",
    "\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "np.random.seed(42)\r\n",
    "\r\n",
    "n_samples, n_features = 200, 50\r\n",
    "x = np.random.randn(n_samples, n_features)\r\n",
    "true_coef = 3 * np.random.randn(n_features)\r\n",
    "\r\n",
    "# Threshold coefficients to render them non-negative\r\n",
    "true_coef[true_coef < 0] = 0\r\n",
    "y = np.dot(x, true_coef)\r\n",
    "\r\n",
    "# Add some noise\r\n",
    "y += 5 * np.random.normal(size=(n_samples, ))\r\n",
    "\r\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\r\n",
    "\r\n",
    "reg_nnls = LinearRegression(positive=True)\r\n",
    "y_pred_nnls = reg_nnls.fit(x_train, y_train).predict(x_test)\r\n",
    "r2_score_nnls = r2_score(y_test, y_pred_nnls)\r\n",
    "print(\"NNLS R2 score\", r2_score_nnls)\r\n",
    "\r\n",
    "reg_ols = LinearRegression()\r\n",
    "y_pred_ols = reg_ols.fit(x_train, y_train).predict(x_test)\r\n",
    "r2_score_ols = r2_score(y_test, y_pred_ols)\r\n",
    "print(\"OLS R2 score\", r2_score_ols)\r\n",
    "\r\n",
    "fig, ax = plt.subplots()\r\n",
    "ax.plot(reg_ols.coef_, reg_nnls.coef_, linewidth=0, marker=\".\")\r\n",
    "\r\n",
    "low_x, high_x = ax.get_xlim()\r\n",
    "low_y, high_y = ax.get_ylim()\r\n",
    "low = max(low_x, low_y)\r\n",
    "high = min(high_x, high_y)\r\n",
    "ax.plot([low, high], [low, high], ls=\"--\", c=\".3\", alpha=.5)\r\n",
    "ax.set_xlabel(\"OLS regression coefficients\", fontweight=\"bold\")\r\n",
    "ax.set_ylabel(\"NNLS regression coefficients\", fontweight=\"bold\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit (system)"
  },
  "interpreter": {
   "hash": "d4be04134da666ba417300a0c020abeb8478be2ead96ebfe9eeadc929de095c9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
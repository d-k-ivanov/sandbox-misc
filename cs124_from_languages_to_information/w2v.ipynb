{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.preprocessing import sequence\r\n",
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras.utils import np_utils\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def tokenize(corpus):\r\n",
    "    \"\"\"\r\n",
    "    Tokenize the corpus of text.\r\n",
    "    :param corpus: list containing a string of text (example: [\"I like playing football with my friends\"])\r\n",
    "    :return corpus_tokenized: indexed list of words in the corpus, in the same order as the original corpus (the example above would return [[1, 2, 3, 4]])\r\n",
    "    :return V: size of vocabulary\r\n",
    "    \"\"\"\r\n",
    "    tokenizer = Tokenizer()\r\n",
    "    tokenizer.fit_on_texts(corpus)\r\n",
    "    corpus_tokenized = tokenizer.texts_to_sequences(corpus)\r\n",
    "    V = len(tokenizer.word_index)\r\n",
    "    return corpus_tokenized, V\r\n",
    "\r\n",
    "\r\n",
    "def initialize(V, N):\r\n",
    "    \"\"\"\r\n",
    "    Initialize the weights of the neural network.\r\n",
    "    :param V: size of the vocabulary\r\n",
    "    :param N: size of the hidden layer\r\n",
    "    :return: weights W1, W2\r\n",
    "    \"\"\"\r\n",
    "    np.random.seed(100)\r\n",
    "    W1 = np.random.rand(V, N)\r\n",
    "    W2 = np.random.rand(N, V)\r\n",
    "\r\n",
    "    return W1, W2\r\n",
    "\r\n",
    "\r\n",
    "def corpus2io(corpus_tokenized, V, window_size):\r\n",
    "    \"\"\"Converts corpus text into context and center words\r\n",
    "    # Arguments\r\n",
    "        corpus_tokenized: corpus text\r\n",
    "        window_size: size of context window\r\n",
    "    # Returns\r\n",
    "        context and center words (arrays)\r\n",
    "    \"\"\"\r\n",
    "    for words in corpus_tokenized:\r\n",
    "        L = len(words)\r\n",
    "        for index, word in enumerate(words):\r\n",
    "            contexts = []\r\n",
    "            center = []\r\n",
    "            s = index - window_size\r\n",
    "            e = index + window_size + 1\r\n",
    "            contexts = contexts + [words[i]-1 for i in range(s, e) if 0 <= i < L and i != index]\r\n",
    "            center.append(word-1)\r\n",
    "            # x has shape c x V where c is size of contexts\r\n",
    "            x = np_utils.to_categorical(contexts, V)\r\n",
    "            # y has shape k x V where k is number of center words\r\n",
    "            y = np_utils.to_categorical(center, V)\r\n",
    "            yield (x, y)\r\n",
    "\r\n",
    "\r\n",
    "def softmax(x):\r\n",
    "    \"\"\"Calculate softmax based probability for given input vector\r\n",
    "    # Arguments\r\n",
    "        x: numpy array/list\r\n",
    "    # Returns\r\n",
    "        softmax of input array\r\n",
    "    \"\"\"\r\n",
    "    e_x = np.exp(x - np.max(x))\r\n",
    "    return e_x / e_x.sum(axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Word2Vec:\r\n",
    "    \"\"\"\r\n",
    "    Python implementation of Word2Vec.\r\n",
    "    # Arguments\r\n",
    "        method : `str`\r\n",
    "            choose method for word2vec (options: 'cbow', 'skipgram')\r\n",
    "            [default: 'cbow']\r\n",
    "        window_size: `integer`\r\n",
    "            size of window [default: 1]\r\n",
    "        n_hidden: `integer`\r\n",
    "            size of hidden layer [default: 2]\r\n",
    "        n_epochs: `integer`\r\n",
    "            number of epochs [default: 1]\r\n",
    "        learning_rate: `float` [default: 0.1]\r\n",
    "        corpus: `str`\r\n",
    "            corpus text\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, method='cbow', window_size=1, n_hidden=2, n_epochs=1, corpus='', learning_rate=0.1):\r\n",
    "        self.window = window_size\r\n",
    "        self.N = n_hidden\r\n",
    "        self.n_epochs = n_epochs\r\n",
    "        self.corpus = [corpus]\r\n",
    "        self.eta = learning_rate\r\n",
    "        if method == 'cbow':\r\n",
    "            self.method = self.cbow\r\n",
    "        elif method == 'skipgram':\r\n",
    "            self.method = self.skipgram\r\n",
    "        else:\r\n",
    "            raise ValueError(\"Method not recognized. Aborting.\")\r\n",
    "\r\n",
    "    def cbow(self, context, label, W1, W2, loss):\r\n",
    "        \"\"\"\r\n",
    "        Implementation of Continuous-Bag-of-Words Word2Vec model\r\n",
    "        :param context: all the context words (these represent the inputs)\r\n",
    "        :param label: the center word (this represents the label)\r\n",
    "        :param W1: weights from the input to the hidden layer\r\n",
    "        :param W2: weights from the hidden to the output layer\r\n",
    "        :param loss: float that represents the current value of the loss function\r\n",
    "        :return: updated weights and loss\r\n",
    "        \"\"\"\r\n",
    "        # context is 'x' from tokenizer, it is a c x V matrix\r\n",
    "        # label is 'y' from tokenizer, it is a 1 x V matrix\r\n",
    "\r\n",
    "        x = np.matrix(np.mean(context, axis=0))\r\n",
    "\r\n",
    "        # x is a 1 x V matrix\r\n",
    "        # W1 is a VxN matrix\r\n",
    "        # h is a N x 1 matrix\r\n",
    "        h = np.matmul(W1.T, x.T)\r\n",
    "\r\n",
    "        # u is a V x 1 matrix\r\n",
    "        u = np.matmul(W2.T, h)\r\n",
    "\r\n",
    "        # W2 is an N x V matrix\r\n",
    "        # y_pred is a V x 1 matrix\r\n",
    "        y_pred = softmax(u)\r\n",
    "        # e is a V x 1 matrix\r\n",
    "        e = -label.T + y_pred\r\n",
    "        # h is N x 1 and e is V x 1 so dW2 is N x V\r\n",
    "        dW2 = np.outer(h, e)\r\n",
    "        # x.T is a V x 1 matrix, W2e is a Nx1 so dW1 this is V x N\r\n",
    "        dW1 = np.outer(x.T, np.matmul(W2, e))\r\n",
    "\r\n",
    "        new_W1 = W1 - self.eta * dW1\r\n",
    "        new_W2 = W2 - self.eta * dW2\r\n",
    "\r\n",
    "        # label is a 1xV matrix so label.T is a Vx1 matrix\r\n",
    "        loss += -float(u[label.T == 1]) + np.log(np.sum(np.exp(u)))\r\n",
    "\r\n",
    "        return new_W1, new_W2, loss\r\n",
    "\r\n",
    "    def skipgram(self, context, x, W1, W2, loss):\r\n",
    "        \"\"\"\r\n",
    "        Implementation of Skip-Gram Word2Vec model\r\n",
    "        :param context: all the context words (these represent the labels)\r\n",
    "        :param x: the center word (this represents the input)\r\n",
    "        :param W1: weights from the input to the hidden layer\r\n",
    "        :param W2: weights from the hidden to the output layer\r\n",
    "        :param loss: float that represents the current value of the loss function\r\n",
    "        :return: updated weights and loss\r\n",
    "        \"\"\"\r\n",
    "        # context is \"x\" from tokenizer, it is a c x V matrix\r\n",
    "        # \"x\" is \"y\" from tokenizer, it is a 1 x V matrix\r\n",
    "        # W1 has dimension V x N (N= number of features, V = vocab size)\r\n",
    "        # x has dimension V x 1\r\n",
    "        h = np.matmul(W1.T, x.T)\r\n",
    "        # h has dimension N x 1\r\n",
    "        # W2 has dimension N x V\r\n",
    "        # u has dimension V x 1\r\n",
    "        u = np.dot(W2.T, h)\r\n",
    "        # y_pred has dimension V x 1\r\n",
    "        y_pred = softmax(u)\r\n",
    "\r\n",
    "        # context is a c by V matrix\r\n",
    "        # e is a V x c matrix\r\n",
    "        e = np.outer(y_pred,np.array([1]*context.shape[0]))-context.T\r\n",
    "\r\n",
    "        # np.sum(e, axis=1) is a V x 1 vectors\r\n",
    "        # h is an N x 1 Vector\r\n",
    "        # dW2 is a N x V matrix\r\n",
    "        dW2 = np.outer(h, np.sum(e, axis=1))\r\n",
    "        # x is a V x 1 matrix\r\n",
    "        # np.dot(W2, np.sum(e,axis=1)) is a product (N x V) (Vx 1) is Nx1\r\n",
    "        # dW1 is an V x N matrix\r\n",
    "        dW1 = np.outer(x, np.dot(W2, np.sum(e, axis=1)))\r\n",
    "\r\n",
    "        new_W1 = W1 - self.eta * dW1\r\n",
    "        new_W2 = W2 - self.eta * dW2\r\n",
    "\r\n",
    "        loss += - np.sum([u[label.T == 1] for label in context]) + len(context) * np.log(np.sum(np.exp(u)))\r\n",
    "\r\n",
    "        return new_W1, new_W2, loss\r\n",
    "\r\n",
    "    def predict(self, x, W1, W2):\r\n",
    "        \"\"\"Predict output from input data and weights\r\n",
    "        :param x: input data\r\n",
    "        :param W1: weights from input to hidden layer\r\n",
    "        :param W2: weights from hidden layer to output layer\r\n",
    "        :return: output of neural network\r\n",
    "        \"\"\"\r\n",
    "        h = np.mean([np.matmul(W1.T, xx) for xx in x], axis=0)\r\n",
    "        u = np.dot(W2.T, h)\r\n",
    "        return softmax(u)\r\n",
    "\r\n",
    "    def run(self):\r\n",
    "        \"\"\"\r\n",
    "        Main method of the Word2Vec class.\r\n",
    "        :return: the final values of the weights W1, W2 and a history of the value of the loss function vs. epoch\r\n",
    "        \"\"\"\r\n",
    "        if len(self.corpus) == 0:\r\n",
    "            raise ValueError('You need to specify a corpus of text.')\r\n",
    "\r\n",
    "        corpus_tokenized, V = tokenize(self.corpus)\r\n",
    "        W1, W2 = initialize(V, self.N)\r\n",
    "\r\n",
    "        loss_vs_epoch = []\r\n",
    "        for e in range(self.n_epochs):\r\n",
    "            loss = 0.\r\n",
    "            for context, center in corpus2io(corpus_tokenized, V, self.window):\r\n",
    "                W1, W2, loss = self.method(context, center, W1, W2, loss)\r\n",
    "            loss_vs_epoch.append(loss)\r\n",
    "\r\n",
    "        return W1, W2, loss_vs_epoch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "window_size = 2\r\n",
    "corpus = ['the quick brown fox jumped over the lazy dog']\r\n",
    "corpus_tokenized, V = tokenize(corpus)\r\n",
    "for i, (x, y) in enumerate(corpus2io(corpus_tokenized, V, window_size)):\r\n",
    "    print(i, \"\\n center word =\", y, \"\\n context words =\\n\",x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = [-1, -5, 1, 5, 3]\r\n",
    "print(('softmax(data) = [' + 4*'{:.4e}  ' + '{:.4e}]').format(*softmax(data)))\r\n",
    "print('sum(softmax)  = {:.2f}'.format(np.sum(softmax(data))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "corpus = 'the quick brown fox jumped over the lazy dog'\r\n",
    "skipgram = Word2Vec(method=\"skipgram\", corpus=corpus,\r\n",
    "                window_size=1, n_hidden=2,\r\n",
    "                n_epochs=600, learning_rate=0.1)\r\n",
    "W1, W2, loss_vs_epoch = skipgram.run()\r\n",
    "\r\n",
    "print(W1)\r\n",
    "print(W2)\r\n",
    "# print(loss_vs_epoch)\r\n",
    "# plt.yticks(range(0,24,2))\r\n",
    "# plt.ylim([6,24])\r\n",
    "plt.plot(loss_vs_epoch)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus = 'the quick brown fox jumped over the lazy dog'\r\n",
    "cbow = Word2Vec(method=\"cbow\", corpus=corpus,\r\n",
    "                window_size=2, n_hidden=2,\r\n",
    "                n_epochs=600, learning_rate=0.1)\r\n",
    "W1, W2, loss_vs_epoch = cbow.run()\r\n",
    "\r\n",
    "print(W1)\r\n",
    "print(W2)\r\n",
    "plt.yticks(range(0,24,2))\r\n",
    "plt.ylim([0,15])\r\n",
    "plt.plot(loss_vs_epoch)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit (system)"
  },
  "interpreter": {
   "hash": "d4be04134da666ba417300a0c020abeb8478be2ead96ebfe9eeadc929de095c9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
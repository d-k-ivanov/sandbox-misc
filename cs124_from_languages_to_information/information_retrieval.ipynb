{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "\r\n",
    "import nltk\r\n",
    "import os\r\n",
    "import string\r\n",
    "import numpy as np\r\n",
    "import copy\r\n",
    "import pandas as pd\r\n",
    "import pickle\r\n",
    "\r\n",
    "# nltk.download()\r\n",
    "nltk.download('punkt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def remove_header(data):\r\n",
    "    try:\r\n",
    "        ind = data.index('\\n\\n')\r\n",
    "        data = data[ind:]\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "        # print(\"No Header\")\r\n",
    "    return data\r\n",
    "\r\n",
    "\r\n",
    "def convert_lower_case(data):\r\n",
    "    return np.char.lower(data)\r\n",
    "\r\n",
    "\r\n",
    "def remove_stop_words(data):\r\n",
    "    stop_words = stopwords.words('english')\r\n",
    "    words = word_tokenize(str(data))\r\n",
    "    new_text = \"\"\r\n",
    "    for w in words:\r\n",
    "        if w not in stop_words:\r\n",
    "            new_text = new_text + \" \" + w\r\n",
    "    return np.char.strip(new_text)\r\n",
    "\r\n",
    "\r\n",
    "def remove_punctuation(data):\r\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\r\n",
    "    for i in range(len(symbols)):\r\n",
    "        data = np.char.replace(data, symbols[i], ' ')\r\n",
    "        data = np.char.replace(data, \"  \", \" \")\r\n",
    "    data = np.char.replace(data, ',', '')\r\n",
    "    return data\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def remove_apostrophe(data):\r\n",
    "    return np.char.replace(data, \"'\", \"\")\r\n",
    "\r\n",
    "\r\n",
    "def remove_single_characters(data):\r\n",
    "    words = word_tokenize(str(data))\r\n",
    "    new_text = \"\"\r\n",
    "    for w in words:\r\n",
    "        if len(w) > 1:\r\n",
    "            new_text = new_text + \" \" + w\r\n",
    "    return np.char.strip(new_text)\r\n",
    "\r\n",
    "\r\n",
    "def convert_numbers(data):\r\n",
    "    data = np.char.replace(data, \"0\", \" zero \")\r\n",
    "    data = np.char.replace(data, \"1\", \" one \")\r\n",
    "    data = np.char.replace(data, \"2\", \" two \")\r\n",
    "    data = np.char.replace(data, \"3\", \" three \")\r\n",
    "    data = np.char.replace(data, \"4\", \" four \")\r\n",
    "    data = np.char.replace(data, \"5\", \" five \")\r\n",
    "    data = np.char.replace(data, \"6\", \" six \")\r\n",
    "    data = np.char.replace(data, \"7\", \" seven \")\r\n",
    "    data = np.char.replace(data, \"8\", \" eight \")\r\n",
    "    data = np.char.replace(data, \"9\", \" nine \")\r\n",
    "    return data\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def stemming(data):\r\n",
    "    stemmer= PorterStemmer()\r\n",
    "    tokens = word_tokenize(str(data))\r\n",
    "    new_text = \"\"\r\n",
    "    for w in tokens:\r\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\r\n",
    "    return np.char.strip(new_text)\r\n",
    "\r\n",
    "\r\n",
    "def preprocess(data, query):\r\n",
    "    if not query:\r\n",
    "        data = remove_header(data)\r\n",
    "    data = convert_lower_case(data)\r\n",
    "    data = convert_numbers(data)\r\n",
    "    data = remove_punctuation(data) #remove comma seperately\r\n",
    "    data = remove_apostrophe(data)\r\n",
    "    data = remove_single_characters(data)\r\n",
    "    data = stemming(data)\r\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "postings = pd.DataFrame()\r\n",
    "frequency = pd.DataFrame()\r\n",
    "doc = 0\r\n",
    "\r\n",
    "# with open(\"./corpuses/sonnets.txt\", 'r') as f:\r\n",
    "    # text = f.read().strip()\r\n",
    "\r\n",
    "for sonnet in os.listdir('./corpuses/sonnets/'):\r\n",
    "    with open('./corpuses/sonnets/' + sonnet, 'r') as f:\r\n",
    "        text = f.read().strip()\r\n",
    "    preprocessed_text = preprocess(text, False)\r\n",
    "    if doc%100 == 0:\r\n",
    "        print(doc)\r\n",
    "\r\n",
    "    tokens = word_tokenize(str(preprocessed_text))\r\n",
    "\r\n",
    "    pos = 0\r\n",
    "    for token in tokens:\r\n",
    "        if token in postings:\r\n",
    "            p = postings[token][0]\r\n",
    "\r\n",
    "            k = [a[0] for a in p]\r\n",
    "            if doc in k:\r\n",
    "                for a in p:\r\n",
    "                    if a[0] == doc:\r\n",
    "                        a[1].add(pos)\r\n",
    "            else:\r\n",
    "                p.append([doc,{pos}])\r\n",
    "                frequency[token][0] += 1\r\n",
    "        else:\r\n",
    "            postings.insert(value=[[[doc, {pos}]]], loc=0, column=token)\r\n",
    "            frequency.insert(value=[1], loc=0, column=token)\r\n",
    "\r\n",
    "        pos += 1\r\n",
    "    doc += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(postings)\r\n",
    "print(frequency)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_word_postings(word):\r\n",
    "    preprocessed_word = str(preprocess(word, True))\r\n",
    "    print(preprocessed_word)\r\n",
    "    print(\"Frequency:\",frequency[preprocessed_word][0])\r\n",
    "    print(\"Postings List:\",postings[preprocessed_word][0])\r\n",
    "\r\n",
    "\r\n",
    "def get_positions(posting_values, doc):\r\n",
    "    for posting_value in posting_values:\r\n",
    "        if posting_value[0] == doc:\r\n",
    "            return posting_value[1]\r\n",
    "    return {}\r\n",
    "\r\n",
    "\r\n",
    "def gen_init_set_matchings(word):\r\n",
    "    init = []\r\n",
    "    word_postings = postings[word][0]\r\n",
    "    for word_posting in word_postings:\r\n",
    "        for positions in word_posting[1]:\r\n",
    "            init.append((word_posting[0], positions))\r\n",
    "    return init\r\n",
    "\r\n",
    "\r\n",
    "def match_positional_index(init, b):\r\n",
    "    matched_docs = []\r\n",
    "    for p in init:\r\n",
    "        doc = p[0]\r\n",
    "        pos = p[1]\r\n",
    "\r\n",
    "        count = 0\r\n",
    "\r\n",
    "        for k in b:\r\n",
    "            pos = pos+1\r\n",
    "            k_pos = postings[k][0]\r\n",
    "            docs_list = [z[0] for z in k_pos]\r\n",
    "            if doc in docs_list:\r\n",
    "                doc_positions = get_positions(k_pos, doc)\r\n",
    "                if pos in doc_positions:\r\n",
    "                    count += 1\r\n",
    "                else:\r\n",
    "                    count += 1\r\n",
    "                    break\r\n",
    "\r\n",
    "            if count == len(b):\r\n",
    "                matched_docs.append(p[0])\r\n",
    "    return set(matched_docs)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def run_query(query):\r\n",
    "    processed_query = preprocess(query, True)\r\n",
    "    print(processed_query)\r\n",
    "\r\n",
    "    query_tokens = word_tokenize(str(processed_query))\r\n",
    "    print(query_tokens)\r\n",
    "\r\n",
    "    if len(query_tokens)==1:\r\n",
    "        print(\"Total Document Mathces\", [a[0] for a in postings[query][0]])\r\n",
    "        return [a[0] for a in postings[query][0]]\r\n",
    "\r\n",
    "    init_word = query_tokens[0]\r\n",
    "    init_matches = gen_init_set_matchings(init_word)\r\n",
    "\r\n",
    "    query_tokens.pop(0)\r\n",
    "    total_matched_docs = match_positional_index(init_matches, query_tokens)\r\n",
    "    print(\"Total Document Matches:\", total_matched_docs)\r\n",
    "    return total_matched_docs\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def print_document(document):\r\n",
    "    with open('./corpuses/sonnets/' + document, 'r', encoding='utf-8') as f:\r\n",
    "        out_text = f.read()\r\n",
    "    print(out_text)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_word_postings(\"lively\")\r\n",
    "get_word_postings(\"king\")\r\n",
    "get_word_postings(\"time\")\r\n",
    "get_word_postings(\"one\")\r\n",
    "get_word_postings(\"be\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "query = \"to be\"\r\n",
    "print(run_query(query))\r\n",
    "print(run_query(\"thee partake\"))\r\n",
    "print(run_query(\"raised love\"))\r\n",
    "print(run_query(\"liquid prisoner\"))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print_document('120')\r\n",
    "print_document('4')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit (system)"
  },
  "interpreter": {
   "hash": "d4be04134da666ba417300a0c020abeb8478be2ead96ebfe9eeadc929de095c9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "\r\n",
    "import nltk\r\n",
    "import os\r\n",
    "import string\r\n",
    "import numpy as np\r\n",
    "import copy\r\n",
    "import pandas as pd\r\n",
    "import pickle\r\n",
    "\r\n",
    "# nltk.download()\r\n",
    "nltk.download('punkt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dkiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def remove_header(data):\r\n",
    "    try:\r\n",
    "        ind = data.index('\\n\\n')\r\n",
    "        data = data[ind:]\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "        # print(\"No Header\")\r\n",
    "    return data\r\n",
    "\r\n",
    "\r\n",
    "def convert_lower_case(data):\r\n",
    "    return np.char.lower(data)\r\n",
    "\r\n",
    "\r\n",
    "def remove_stop_words(data):\r\n",
    "    stop_words = stopwords.words('english')\r\n",
    "    words = word_tokenize(str(data))\r\n",
    "    new_text = \"\"\r\n",
    "    for w in words:\r\n",
    "        if w not in stop_words:\r\n",
    "            new_text = new_text + \" \" + w\r\n",
    "    return np.char.strip(new_text)\r\n",
    "\r\n",
    "\r\n",
    "def remove_punctuation(data):\r\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\r\n",
    "    for i in range(len(symbols)):\r\n",
    "        data = np.char.replace(data, symbols[i], ' ')\r\n",
    "        data = np.char.replace(data, \"  \", \" \")\r\n",
    "    data = np.char.replace(data, ',', '')\r\n",
    "    return data\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def remove_apostrophe(data):\r\n",
    "    return np.char.replace(data, \"'\", \"\")\r\n",
    "\r\n",
    "\r\n",
    "def remove_single_characters(data):\r\n",
    "    words = word_tokenize(str(data))\r\n",
    "    new_text = \"\"\r\n",
    "    for w in words:\r\n",
    "        if len(w) > 1:\r\n",
    "            new_text = new_text + \" \" + w\r\n",
    "    return np.char.strip(new_text)\r\n",
    "\r\n",
    "\r\n",
    "def convert_numbers(data):\r\n",
    "    data = np.char.replace(data, \"0\", \" zero \")\r\n",
    "    data = np.char.replace(data, \"1\", \" one \")\r\n",
    "    data = np.char.replace(data, \"2\", \" two \")\r\n",
    "    data = np.char.replace(data, \"3\", \" three \")\r\n",
    "    data = np.char.replace(data, \"4\", \" four \")\r\n",
    "    data = np.char.replace(data, \"5\", \" five \")\r\n",
    "    data = np.char.replace(data, \"6\", \" six \")\r\n",
    "    data = np.char.replace(data, \"7\", \" seven \")\r\n",
    "    data = np.char.replace(data, \"8\", \" eight \")\r\n",
    "    data = np.char.replace(data, \"9\", \" nine \")\r\n",
    "    return data\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def stemming(data):\r\n",
    "    stemmer= PorterStemmer()\r\n",
    "    tokens = word_tokenize(str(data))\r\n",
    "    new_text = \"\"\r\n",
    "    for w in tokens:\r\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\r\n",
    "    return np.char.strip(new_text)\r\n",
    "\r\n",
    "\r\n",
    "def preprocess(data, query):\r\n",
    "    if not query:\r\n",
    "        data = remove_header(data)\r\n",
    "    data = convert_lower_case(data)\r\n",
    "    data = convert_numbers(data)\r\n",
    "    data = remove_punctuation(data) #remove comma seperately\r\n",
    "    data = remove_apostrophe(data)\r\n",
    "    data = remove_single_characters(data)\r\n",
    "    data = stemming(data)\r\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "postings = pd.DataFrame()\r\n",
    "frequency = pd.DataFrame()\r\n",
    "doc = 0\r\n",
    "\r\n",
    "# with open(\"./corpuses/sonnets.txt\", 'r') as f:\r\n",
    "    # text = f.read().strip()\r\n",
    "\r\n",
    "for sonnet in os.listdir('./corpuses/sonnets/'):\r\n",
    "    with open('./corpuses/sonnets/' + sonnet, 'r') as f:\r\n",
    "        text = f.read().strip()\r\n",
    "    preprocessed_text = preprocess(text, False)\r\n",
    "    if doc%100 == 0:\r\n",
    "        print(doc)\r\n",
    "\r\n",
    "    tokens = word_tokenize(str(preprocessed_text))\r\n",
    "\r\n",
    "    pos = 0\r\n",
    "    for token in tokens:\r\n",
    "        if token in postings:\r\n",
    "            p = postings[token][0]\r\n",
    "\r\n",
    "            k = [a[0] for a in p]\r\n",
    "            if doc in k:\r\n",
    "                for a in p:\r\n",
    "                    if a[0] == doc:\r\n",
    "                        a[1].add(pos)\r\n",
    "            else:\r\n",
    "                p.append([doc,{pos}])\r\n",
    "                frequency[token][0] += 1\r\n",
    "        else:\r\n",
    "            postings.insert(value=[[[doc, {pos}]]], loc=0, column=token)\r\n",
    "            frequency.insert(value=[1], loc=0, column=token)\r\n",
    "\r\n",
    "        pos += 1\r\n",
    "    doc += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "100\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# print(postings)\r\n",
    "print(frequency)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   discas  cool  quench  disarm  virgin  legion  votari  trip  chast  nymph  \\\n",
      "0       1     1       1       1       1       1       1     1      1      1   \n",
      "\n",
      "   ...  beauti  therebi  that  increas  desir  we  creatur  fairest  from  one  \n",
      "0  ...      43        2   131        5     13  11        3        4    61   88  \n",
      "\n",
      "[1 rows x 2431 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def get_word_postings(word):\r\n",
    "    preprocessed_word = str(preprocess(word, True))\r\n",
    "    print(preprocessed_word)\r\n",
    "    print(\"Frequency:\",frequency[preprocessed_word][0])\r\n",
    "    print(\"Postings List:\",postings[preprocessed_word][0])\r\n",
    "\r\n",
    "\r\n",
    "def get_positions(posting_values, doc):\r\n",
    "    for posting_value in posting_values:\r\n",
    "        if posting_value[0] == doc:\r\n",
    "            return posting_value[1]\r\n",
    "    return {}\r\n",
    "\r\n",
    "\r\n",
    "def gen_init_set_matchings(word):\r\n",
    "    init = []\r\n",
    "    word_postings = postings[word][0]\r\n",
    "    for word_posting in word_postings:\r\n",
    "        for positions in word_posting[1]:\r\n",
    "            init.append((word_posting[0], positions))\r\n",
    "    return init\r\n",
    "\r\n",
    "\r\n",
    "def match_positional_index(init, b):\r\n",
    "    matched_docs = []\r\n",
    "    for p in init:\r\n",
    "        doc = p[0]\r\n",
    "        pos = p[1]\r\n",
    "\r\n",
    "        count = 0\r\n",
    "\r\n",
    "        for k in b:\r\n",
    "            pos = pos+1\r\n",
    "            k_pos = postings[k][0]\r\n",
    "            docs_list = [z[0] for z in k_pos]\r\n",
    "            if doc in docs_list:\r\n",
    "                doc_positions = get_positions(k_pos, doc)\r\n",
    "                if pos in doc_positions:\r\n",
    "                    count += 1\r\n",
    "                else:\r\n",
    "                    count += 1\r\n",
    "                    break\r\n",
    "\r\n",
    "            if count == len(b):\r\n",
    "                matched_docs.append(p[0])\r\n",
    "    return set(matched_docs)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def run_query(query):\r\n",
    "    processed_query = preprocess(query, True)\r\n",
    "    print(processed_query)\r\n",
    "\r\n",
    "    query_tokens = word_tokenize(str(processed_query))\r\n",
    "    print(query_tokens)\r\n",
    "\r\n",
    "    if len(query_tokens)==1:\r\n",
    "        print(\"Total Document Mathces\", [a[0] for a in postings[query][0]])\r\n",
    "        return [a[0] for a in postings[query][0]]\r\n",
    "\r\n",
    "    init_word = query_tokens[0]\r\n",
    "    init_matches = gen_init_set_matchings(init_word)\r\n",
    "\r\n",
    "    query_tokens.pop(0)\r\n",
    "    total_matched_docs = match_positional_index(init_matches, query_tokens)\r\n",
    "    print(\"Total Document Matches:\", total_matched_docs)\r\n",
    "    return total_matched_docs\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def print_document(document):\r\n",
    "    with open('./corpuses/sonnets/' + document, 'r', encoding='utf-8') as f:\r\n",
    "        out_text = f.read()\r\n",
    "    print(out_text)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "get_word_postings(\"lively\")\r\n",
    "get_word_postings(\"king\")\r\n",
    "get_word_postings(\"time\")\r\n",
    "get_word_postings(\"one\")\r\n",
    "get_word_postings(\"be\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "live\n",
      "Frequency: 37\n",
      "Postings List: [[2, {104}], [3, {57, 99}], [4, {104}], [5, {91}], [9, {111}], [10, {39}], [12, {19}], [15, {106, 52, 87}], [16, {115}], [17, {105}], [18, {115, 31}], [21, {55}], [30, {69}], [34, {30}], [35, {46}], [36, {92}], [38, {43}], [42, {83}], [53, {34, 74}], [54, {56, 102}], [62, {102}], [66, {8, 72, 42, 86, 61}], [67, {32, 49, 13}], [71, {90, 12}], [78, {96}], [80, {4, 101}], [82, {94}], [92, {4, 36}], [93, {76}], [104, {98}], [106, {85}], [123, {109}], [126, {64}], [127, {94}], [143, {97}], [145, {65}], [152, {41}]]\n",
      "king\n",
      "Frequency: 4\n",
      "Postings List: [[28, {113}], [62, {49}], [86, {110}], [114, {50}]]\n",
      "time\n",
      "Frequency: 46\n",
      "Postings List: [[0, {20}], [2, {100, 14}], [4, {35}], [5, {56, 64, 77}], [10, {57}], [11, {10, 101, 79}], [14, {97, 79}], [15, {71, 15}], [16, {8, 112}], [17, {90}], [18, {104, 51, 3}], [21, {24}], [29, {31}], [31, {42}], [36, {107}], [37, {70}], [38, {91, 85}], [43, {98}], [46, {54}], [48, {8, 67, 4, 36}], [51, {68}], [54, {30}], [56, {21, 14}], [57, {16, 79}], [59, {62, 96, 54}], [62, {66, 11}], [63, {85, 6}], [64, {67, 60, 71}], [69, {47}], [72, {3}], [75, {21}], [76, {58}], [81, {57}], [96, {33, 37}], [99, {101, 91, 77, 47}], [105, {72, 9}], [106, {70}], [107, {107}], [108, {56, 52}], [114, {38, 71}], [115, {64}], [116, {42}], [119, {48}], [122, {4}], [123, {27, 61, 102, 23}], [125, {60, 13}]]\n",
      "one\n",
      "Frequency: 88\n",
      "Postings List: [[0, {0}], [5, {62}], [7, {91, 90, 67, 102}], [9, {0}], [10, {0, 1, 13}], [11, {0}], [12, {0}], [13, {0}], [14, {0}], [15, {0}], [16, {0}], [17, {0}], [18, {0, 64}], [19, {87}], [20, {1}], [21, {18}], [27, {49}], [28, {37}], [30, {1}], [32, {61, 79}], [35, {16, 41}], [38, {105, 52}], [40, {1}], [41, {116}], [50, {1}], [52, {70, 20, 23, 24, 29}], [60, {1}], [70, {1}], [75, {38}], [80, {1}], [82, {98}], [90, {1, 63}], [98, {65}], [99, {0}], [100, {0, 2}], [101, {0}], [102, {0}], [103, {0}], [104, {0, 108, 54, 87, 26, 28}], [105, {0}], [106, {0}], [107, {0}], [108, {0}], [109, {0, 1}], [110, {0, 1, 2}], [111, {0, 1}], [112, {0, 1}], [113, {0, 1}], [114, {0, 1}], [115, {0, 1}], [116, {0, 1}], [117, {0, 1}], [118, {0, 1}], [119, {0}], [120, {0, 2}], [121, {0}], [122, {0}], [123, {0}], [124, {0}], [125, {0}], [126, {0}], [127, {0}], [128, {0}], [129, {0}], [130, {0, 2, 87}], [131, {0}], [132, {0}], [133, {0}], [134, {0, 117, 92, 112}], [135, {0, 67, 84, 55}], [136, {0}], [137, {0}], [138, {0}], [139, {0}], [140, {0, 2, 78}], [141, {0}], [142, {0, 10}], [143, {0, 107, 86}], [144, {0}], [145, {0}], [146, {0}], [147, {0}], [148, {0}], [149, {0}], [150, {0, 2}], [151, {0}], [152, {0}], [153, {0}]]\n",
      "be\n",
      "Frequency: 86\n",
      "Postings List: [[0, {96}], [1, {25, 99, 33}], [2, {58, 108}], [3, {81, 93, 21, 103}], [5, {105, 14, 58, 29, 94}], [7, {99}], [8, {36}], [9, {82, 76, 62}], [16, {93, 79}], [19, {103}], [21, {70, 63}], [22, {68}], [24, {102}], [31, {46}], [34, {96, 4}], [35, {9, 105, 31}], [37, {64, 113, 108}], [39, {120, 60}], [40, {105, 43, 36}], [41, {16}], [42, {74}], [43, {24, 67}], [44, {64, 47}], [47, {102}], [49, {66}], [50, {78}], [51, {104, 108}], [55, {97, 35, 7, 73, 105, 15}], [56, {2, 74}], [57, {103, 40, 109, 29, 63}], [58, {88, 4}], [59, {45}], [60, {24}], [62, {97, 6}], [65, {79}], [69, {34, 8, 44, 76, 84}], [70, {58}], [71, {83}], [73, {81, 3, 94}], [74, {57, 99}], [77, {92, 61}], [79, {82, 100}], [80, {84, 69, 86, 31}], [81, {95}], [82, {73, 41, 83, 77}], [83, {106}], [87, {40, 69, 5}], [88, {60}], [90, {89}], [91, {110}], [92, {89}], [95, {48, 112}], [99, {85}], [100, {89, 95, 70, 7}], [103, {10, 98}], [104, {24, 7}], [108, {87}], [113, {8, 99}], [115, {98, 60}], [117, {57, 34, 92}], [120, {97, 6, 14, 18, 82, 87}], [121, {59}], [122, {105, 107}], [123, {18}], [124, {62}], [125, {89}], [127, {66}], [129, {32, 22}], [130, {73, 61}], [131, {25}], [132, {112, 69, 37, 95}], [133, {43, 31}], [134, {84}], [135, {86}], [136, {40, 48}], [137, {113}], [139, {105, 3, 60, 101}], [140, {99, 59}], [141, {101, 117, 69}], [142, {99}], [143, {67, 77, 53}], [145, {88, 91}], [147, {56, 80, 37}], [149, {112}], [150, {92}], [151, {119}]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "query = \"to be\"\r\n",
    "print(run_query(query))\r\n",
    "print(run_query(\"thee partake\"))\r\n",
    "print(run_query(\"raised love\"))\r\n",
    "print(run_query(\"liquid prisoner\"))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "to be\n",
      "['to', 'be']\n",
      "Total Document Matches: {1, 2, 3, 130, 5, 132, 133, 136, 140, 143, 149, 150, 40, 73, 74, 80, 100, 117, 120, 127}\n",
      "{1, 2, 3, 130, 5, 132, 133, 136, 140, 143, 149, 150, 40, 73, 74, 80, 100, 117, 120, 127}\n",
      "thee partak\n",
      "['thee', 'partak']\n",
      "Total Document Matches: {148}\n",
      "{148}\n",
      "rais love\n",
      "['rais', 'love']\n",
      "Total Document Matches: {149}\n",
      "{149}\n",
      "liquid prison\n",
      "['liquid', 'prison']\n",
      "Total Document Matches: {4}\n",
      "{4}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# print_document('120')\r\n",
    "print_document('4')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                     5\n",
      "  Those hours that with gentle work did frame\n",
      "  The lovely gaze where every eye doth dwell\n",
      "  Will play the tyrants to the very same,\n",
      "  And that unfair which fairly doth excel:\n",
      "  For never-resting time leads summer on\n",
      "  To hideous winter and confounds him there,\n",
      "  Sap checked with frost and lusty leaves quite gone,\n",
      "  Beauty o'er-snowed and bareness every where:\n",
      "  Then were not summer's distillation left\n",
      "  A liquid prisoner pent in walls of glass,\n",
      "  Beauty's effect with beauty were bereft,\n",
      "  Nor it nor no remembrance what it was.\n",
      "    But flowers distilled though they with winter meet,\n",
      "    Leese but their show, their substance still lives sweet.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit (system)"
  },
  "interpreter": {
   "hash": "d4be04134da666ba417300a0c020abeb8478be2ead96ebfe9eeadc929de095c9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
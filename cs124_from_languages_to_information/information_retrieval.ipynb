{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "\r\n",
    "import nltk\r\n",
    "import os\r\n",
    "import string\r\n",
    "import numpy as np\r\n",
    "import copy\r\n",
    "import pandas as pd\r\n",
    "import pickle\r\n",
    "\r\n",
    "# nltk.download()\r\n",
    "nltk.download('punkt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dkiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def remove_header(data):\r\n",
    "    try:\r\n",
    "        ind = data.index('\\n\\n')\r\n",
    "        data = data[ind:]\r\n",
    "    except:\r\n",
    "        print(\"No Header\")\r\n",
    "    return data\r\n",
    "\r\n",
    "\r\n",
    "def convert_lower_case(data):\r\n",
    "    return np.char.lower(data)\r\n",
    "\r\n",
    "\r\n",
    "def remove_stop_words(data):\r\n",
    "    stop_words = stopwords.words('english')\r\n",
    "    words = word_tokenize(str(data))\r\n",
    "    new_text = \"\"\r\n",
    "    for w in words:\r\n",
    "        if w not in stop_words:\r\n",
    "            new_text = new_text + \" \" + w\r\n",
    "    return np.char.strip(new_text)\r\n",
    "\r\n",
    "\r\n",
    "def remove_punctuation(data):\r\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\r\n",
    "    for i in range(len(symbols)):\r\n",
    "        data = np.char.replace(data, symbols[i], ' ')\r\n",
    "        data = np.char.replace(data, \"  \", \" \")\r\n",
    "    data = np.char.replace(data, ',', '')\r\n",
    "    return data\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def remove_apostrophe(data):\r\n",
    "    return np.char.replace(data, \"'\", \"\")\r\n",
    "\r\n",
    "\r\n",
    "def remove_single_characters(data):\r\n",
    "    words = word_tokenize(str(data))\r\n",
    "    new_text = \"\"\r\n",
    "    for w in words:\r\n",
    "        if len(w) > 1:\r\n",
    "            new_text = new_text + \" \" + w\r\n",
    "    return np.char.strip(new_text)\r\n",
    "\r\n",
    "\r\n",
    "def convert_numbers(data):\r\n",
    "    data = np.char.replace(data, \"0\", \" zero \")\r\n",
    "    data = np.char.replace(data, \"1\", \" one \")\r\n",
    "    data = np.char.replace(data, \"2\", \" two \")\r\n",
    "    data = np.char.replace(data, \"3\", \" three \")\r\n",
    "    data = np.char.replace(data, \"4\", \" four \")\r\n",
    "    data = np.char.replace(data, \"5\", \" five \")\r\n",
    "    data = np.char.replace(data, \"6\", \" six \")\r\n",
    "    data = np.char.replace(data, \"7\", \" seven \")\r\n",
    "    data = np.char.replace(data, \"8\", \" eight \")\r\n",
    "    data = np.char.replace(data, \"9\", \" nine \")\r\n",
    "    return data\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def stemming(data):\r\n",
    "    stemmer= PorterStemmer()\r\n",
    "    tokens = word_tokenize(str(data))\r\n",
    "    new_text = \"\"\r\n",
    "    for w in tokens:\r\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\r\n",
    "    return np.char.strip(new_text)\r\n",
    "\r\n",
    "\r\n",
    "def preprocess(data, query):\r\n",
    "    if not query:\r\n",
    "        data = remove_header(data)\r\n",
    "    data = convert_lower_case(data)\r\n",
    "    data = convert_numbers(data)\r\n",
    "    data = remove_punctuation(data) #remove comma seperately\r\n",
    "    data = remove_apostrophe(data)\r\n",
    "    data = remove_single_characters(data)\r\n",
    "    data = stemming(data)\r\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "postings = pd.DataFrame()\r\n",
    "frequency = pd.DataFrame()\r\n",
    "doc = 0\r\n",
    "\r\n",
    "# with open(\"./corpuses/sonnets.txt\", 'r') as f:\r\n",
    "    # text = f.read().strip()\r\n",
    "\r\n",
    "for sonnet in os.listdir('./corpuses/sonnets/'):\r\n",
    "    with open('./corpuses/sonnets/' + sonnet, 'r') as f:\r\n",
    "        text = f.read().strip()\r\n",
    "    preprocessed_text = preprocess(text, False)\r\n",
    "    if doc%100 == 0:\r\n",
    "        print(doc)\r\n",
    "\r\n",
    "    tokens = word_tokenize(str(preprocessed_text))\r\n",
    "\r\n",
    "    pos = 0\r\n",
    "    for token in tokens:\r\n",
    "        if token in postings:\r\n",
    "            p = postings[token][0]\r\n",
    "\r\n",
    "            k = [a[0] for a in p]\r\n",
    "            if doc in k:\r\n",
    "                for a in p:\r\n",
    "                    if a[0] == doc:\r\n",
    "                        a[1].add(pos)\r\n",
    "            else:\r\n",
    "                p.append([doc,{pos}])\r\n",
    "                frequency[token][0] += 1\r\n",
    "        else:\r\n",
    "            postings.insert(value=[[[doc, {pos}]]], loc=0, column=token)\r\n",
    "            frequency.insert(value=[1], loc=0, column=token)\r\n",
    "\r\n",
    "        pos += 1\r\n",
    "    doc += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No Header\n",
      "0\n",
      "No Header\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\dkiva\\.jpenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3361: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2408/2714803846.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mfrequency\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mpostings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.jpenv\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1054\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m         \u001b[0mcacher_needs_updating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_chained_assignment_possible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.jpenv\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_check_is_chained_assignment_possible\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_view\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_cached\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[0mref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_cacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1211\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mref\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1212\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_setitem_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"referent\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.jpenv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_is_mixed_type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5590\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5592\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5594\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.jpenv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdtypes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5656\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5657\u001b[0m         \"\"\"\n\u001b[1;32m-> 5658\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5659\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.jpenv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mget_dtypes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mdtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblknos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# print(postings)\r\n",
    "print(frequency)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   discas  cool  quench  disarm  virgin  legion  votari  trip  chast  nymph  \\\n",
      "0       1     1       1       1       1       1       1     1      1      1   \n",
      "\n",
      "   ...  beauti  therebi  that  increas  desir  we  creatur  fairest  from  one  \n",
      "0  ...       1        1     1        1      1   1        1        1     1    1  \n",
      "\n",
      "[1 rows x 2431 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def get_word_postings(word):\r\n",
    "    preprocessed_word = str(preprocess(word, True))\r\n",
    "    print(preprocessed_word)\r\n",
    "    print(\"Frequency:\",frequency[preprocessed_word][0])\r\n",
    "    print(\"Postings List:\",postings[preprocessed_word][0])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def get_positions(posting_values, doc):\r\n",
    "    for posting_value in posting_values:\r\n",
    "        if posting_value[0] == doc:\r\n",
    "            return posting_value[1]\r\n",
    "    return {}\r\n",
    "\r\n",
    "\r\n",
    "def gen_init_set_matchings(word):\r\n",
    "    init = []\r\n",
    "    word_postings = postings[word][0]\r\n",
    "    for word_posting in word_postings:\r\n",
    "        for positions in word_posting[1]:\r\n",
    "            init.append((word_posting[0], positions))\r\n",
    "    return init\r\n",
    "\r\n",
    "\r\n",
    "def match_positional_index(init, b):\r\n",
    "    matched_docs = []\r\n",
    "    for p in init:\r\n",
    "        doc = p[0]\r\n",
    "        pos = p[1]\r\n",
    "\r\n",
    "        count = 0\r\n",
    "\r\n",
    "        for k in b:\r\n",
    "            pos = pos+1\r\n",
    "            k_pos = postings[k][0]\r\n",
    "            docs_list = [z[0] for z in k_pos]\r\n",
    "            if doc in docs_list:\r\n",
    "                doc_positions = get_positions(k_pos, doc)\r\n",
    "                if pos in doc_positions:\r\n",
    "                    count += 1\r\n",
    "                else:\r\n",
    "                    count += 1\r\n",
    "                    break\r\n",
    "\r\n",
    "            if count == len(b):\r\n",
    "                matched_docs.append(p[0])\r\n",
    "    return set(matched_docs)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def run_query(query):\r\n",
    "    processed_query = preprocess(query, True)\r\n",
    "    print(processed_query)\r\n",
    "\r\n",
    "    query_tokens = word_tokenize(str(processed_query))\r\n",
    "    print(query_tokens)\r\n",
    "\r\n",
    "    if len(query_tokens)==1:\r\n",
    "        print(\"Total Document Mathces\", [a[0] for a in postings[query][0]])\r\n",
    "        return [a[0] for a in postings[query][0]]\r\n",
    "\r\n",
    "    init_word = query_tokens[0]\r\n",
    "    init_matches = gen_init_set_matchings(init_word)\r\n",
    "\r\n",
    "    query_tokens.pop(0)\r\n",
    "    total_matched_docs = match_positional_index(init_matches, query_tokens)\r\n",
    "    print(\"Total Document Matches:\", total_matched_docs)\r\n",
    "    return total_matched_docs\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def print_file(file):\r\n",
    "    out_file = open(file, 'r', encoding='utf-8')\r\n",
    "    out_text = out_file.read()\r\n",
    "    print(out_text)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "get_word_postings(\"lively\")\r\n",
    "get_word_postings(\"king\")\r\n",
    "get_word_postings(\"time\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "live\n",
      "Frequency: 1\n",
      "Postings List: [[0, {641, 3460, 2054, 397, 17296, 7578, 9373, 4001, 548, 3879, 6055, 7465, 6189, 7597, 10417, 8116, 439, 12090, 1726, 4159, 7614, 9156, 327, 7499, 14027, 16333, 6095, 10449, 1107, 1365, 14296, 2138, 6235, 8922, 7518, 2015, 1761, 9059, 7140, 8038, 14438, 7529, 10603, 11884, 1902, 4847, 4336, 1780, 7543, 2424, 16506, 1151}]]\n",
      "king\n",
      "Frequency: 1\n",
      "Postings List: [[0, {3280, 12969, 9837, 7087}]]\n",
      "time\n",
      "Frequency: 1\n",
      "Postings List: [[0, {12291, 9229, 6163, 20, 11287, 2074, 11301, 11311, 7232, 4174, 2127, 6748, 606, 6756, 614, 1643, 627, 13941, 13945, 1661, 11904, 6790, 1169, 10899, 4247, 7319, 1689, 5273, 13979, 10903, 12957, 7326, 7330, 7845, 12990, 11967, 14020, 1745, 13521, 1240, 6374, 5868, 237, 6381, 3312, 1795, 4378, 1309, 4384, 13094, 12075, 1331, 323, 14148, 8517, 5457, 5461, 2393, 6491, 1899, 5489, 14195, 4982, 13183, 7049, 5520, 6554, 7104, 12227, 2000, 8145, 3545, 8666, 479, 2026, 13808, 7153, 11257, 12287}]]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit (system)"
  },
  "interpreter": {
   "hash": "d4be04134da666ba417300a0c020abeb8478be2ead96ebfe9eeadc929de095c9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}